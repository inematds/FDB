<!doctype html>
<html lang="pt-BR">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width,initial-scale=1"/>
  <title>FDB ‚Äî IA aplicada a dados</title>
  <meta name="description" content="Guia pr√°tico: por que e como usar IA com bancos de dados ‚Äî janela de contexto e RAG vetorial."/>
  <link rel="stylesheet" href="assets/styles.css"/>
</head>
<body>
  <header>
    <nav class="nav">
      <div class="brand"><a href="index.html">FDB</a></div>
      <div class="menu">
        <a href="index.html">In√≠cio</a>
        <a href="iniciante.html">Iniciante</a>
        <a href="tecnico.html">T√©cnico</a>
        <a href="avancado.html">Avan√ßado</a>
        <a class="active" href="ia.html">IA</a>
        <a href="comparativos.html">Comparativos</a>
        <a href="gerencia-informacoes.html">Ger√™ncia</a>
        <a href="exercicios.html">Exerc√≠cios</a>
      </div>
    </nav>
  </header>
  <main>
    <div class="card-head"><span class="icon" aria-hidden="true">ü§ñ</span><h1>IA aplicada a dados</h1></div>
    <p class="muted">Por que: acelerar respostas, resumir dados, construir busca sem√¢ntica e chat sobre conhecimento. Como: escolher janela de contexto adequada e aplicar RAG com vetores.</p>

    <section class="section">
      <h2>Janela de contexto (context window)</h2>
      <ul class="list">
        <li><b>Por que importa:</b> limita o quanto o modelo ‚Äúolha‚Äù de uma vez. Estouro = perda de informa√ß√£o relevante.</li>
        <li><b>Como escolher:</b> estime o tamanho m√©dio dos prompts + dados; use janelas maiores para tarefas com muitos trechos, mas prefira RAG quando o corpus crescer.</li>
        <li><b>Boas pr√°ticas:</b> comprima/estruture entradas, use resumos e metadados; priorize trechos por relev√¢ncia antes de enviar ao modelo.</li>
      </ul>
    </section>

    <section class="section">
      <h2>Weaviate ‚Äî h√≠brido (vetor + BM25)</h2>
      <p>Banco vetorial com busca h√≠brida. Por qu√™: combinar sem√¢ntica e palavras‚Äëchave. Como: upsert com vetor e consulta <code>nearVector</code>.</p>
      <pre><code># Python ‚Äî weaviate-client
import weaviate, os

client = weaviate.WeaviateClient("http://localhost:8080")

# Inser√ß√£o com vetor gerado externamente
vec = embed("Normaliza√ß√£o at√© 3FN reduz redund√¢ncia")
client.collections.get_or_create("documents", vectorizer_config={"none": {}})
client.collections.get("documents").data.insert({
  "content": "Normaliza√ß√£o at√© 3FN reduz redund√¢ncia",
  "metadata": {"topic": "modelagem"}
}, vector=vec)

# Busca sem√¢ntica
q = embed("Como normalizar tabelas?")
res = client.collections.get("documents").query.near_vector(q, limit=5)
context = "\n".join(f"- {o['content']}" for o in res.objects)
answer = llm.generate(f"Contexto:\n{context}\n\nPergunta: Como normalizar tabelas?")
      </code></pre>
    </section>

    <section class="section">
      <h2>Milvus ‚Äî open source em escala</h2>
      <p>Banco vetorial open source. Por qu√™: alto desempenho e ecossistema maduro. Como: cole√ß√£o com campo <code>FLOAT_VECTOR</code>.</p>
      <pre><code># Python ‚Äî pymilvus
from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection

connections.connect(alias="default", host="localhost", port="19530")

fields = [
  FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
  FieldSchema(name="content", dtype=DataType.VARCHAR, max_length=2048),
  FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=1536),
]
schema = CollectionSchema(fields, description="docs")
col = Collection("documents", schema=schema)

# √çndice e carga
col.create_index("embedding", {"index_type":"IVF_FLAT","metric_type":"COSINE","params":{"nlist":1024}})
col.load()

# Inser√ß√£o
emb = embed("√çndices aceleram leitura")
col.insert([[None], ["√çndices aceleram leitura"], [emb]])

# Busca
q = embed("Quando criar um √≠ndice?")
res = col.search([q], "embedding", params={"nprobe":16}, limit=5, output_fields=["content"])
context = "\n".join(f"- {hit.entity.get('content')}" for hit in res[0])
answer = llm.generate(f"Contexto:\n{context}\n\nPergunta: Quando criar um √≠ndice?")
      </code></pre>
    </section>

    <section class="section">
      <h2>Qdrant ‚Äî vetorial com payload</h2>
      <p>Armazena vetores com metadados (payload). Por qu√™: filtros estruturados + sem√¢ntica. Como: cole√ß√£o com <code>cosine</code>.</p>
      <pre><code># Python ‚Äî qdrant-client
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct

qc = QdrantClient(host="localhost", port=6333)
qc.recreate_collection(
  collection_name="documents",
  vectors_config=VectorParams(size=1536, distance=Distance.COSINE)
)

emb = embed("Transa√ß√µes garantem atomicidade")
qc.upsert("documents", [PointStruct(id=1, vector=emb, payload={"content":"Transa√ß√µes garantem atomicidade","topic":"transacoes"})])

q = embed("O que √© ACID?")
res = qc.search("documents", query_vector=q, limit=5)
      </code></pre>
    </section>

    <section class="section">
      <h2>Redis com vetores (RedisVL/FTS)</h2>
      <pre><code># Defini√ß√£o do √≠ndice com campo vetorial
FT.CREATE idx:docs ON HASH PREFIX 1 doc: SCHEMA embedding VECTOR HNSW 6 TYPE FLOAT32 DIM 1536 DISTANCE_METRIC COSINE content TEXT

# Consulta KNN (retorna top‚Äë5 por similaridade)
FT.SEARCH idx:docs '*=>[KNN 5 @embedding $vec AS score]' PARAMS 2 vec &lt;BLOB_QUERY&gt; SORTBY score RETURN 2 content score DIALECT 2
      </code></pre>
      <div class="note">Mantenha embeddings como FLOAT32; para cosseno, normalize vetores na aplica√ß√£o.</div>
    </section>

    <section class="section">
      <h2>Chroma ‚Äî local/dev</h2>
      <p>Armazenamento vetorial leve para desenvolvimento local. Por qu√™: simplicidade. Como: cole√ß√£o com embeddings externos.</p>
      <pre><code># Python ‚Äî chromadb
import chromadb
client = chromadb.Client()
col = client.get_or_create_collection("documents")

emb = embed("√çndices e EXPLAIN")
col.add(documents=["√çndices e EXPLAIN"], embeddings=[emb], ids=["doc-1"], metadatas=[{"topic":"indices"}])

q = embed("Como usar EXPLAIN?")
res = col.query(query_embeddings=[q], n_results=5)
context = "\n".join(f"- {d}" for d in res['documents'][0])
answer = llm.generate(f"Contexto:\n{context}\n\nPergunta: Como usar EXPLAIN?")
      </code></pre>
    </section>

    <section class="section">
      <h2>Azure AI Search ‚Äî √≠ndice vetorial</h2>
      <p>√çndice gerenciado com campos vetoriais. Por qu√™: integra√ß√£o com filtros e ranking. Como: campo <code>vector</code> + skillset opcional.</p>
      <pre><code>{
  "name": "documents",
  "fields": [
    {"name":"id","type":"Edm.String","key":true},
    {"name":"content","type":"Edm.String","searchable":true},
    {"name":"embedding","type":"Collection(Edm.Single)","searchable":true,"vectorSearchDimensions":1536}
  ],
  "vectorSearch": {"algorithmConfigurations": [{"name":"vec-config","kind":"hnsw"}]}
}
      </code></pre>
      <div class="note">Combine busca sem√¢ntica com filtros por metadados e reranking.</div>
    </section>

    <section class="section">
      <h2>RAG com vetores</h2>
      <p>RAG (Retrieval‚ÄëAugmented Generation) injeta conhecimento externo relevante na janela de contexto do modelo. Vetores (embeddings) permitem buscar por similaridade sem√¢ntica.</p>
      <ul class="list">
        <li><b>Por que usar:</b> escalar al√©m da janela, manter respostas atualizadas, reduzir alucina√ß√µes.</li>
        <li><b>Como usar (pipeline):</b> ingest√£o ‚Üí chunking ‚Üí gera√ß√£o de embeddings ‚Üí indexa√ß√£o vetorial ‚Üí consulta ‚Üí reranking opcional ‚Üí montagem do prompt ‚Üí gera√ß√£o.</li>
        <li><b>Ferramentas comuns:</b> pgvector (PostgreSQL), FAISS, Milvus, Elasticsearch/OpenSearch com vetores.</li>
        <li><b>Dicas:</b> escolha tamanho de chunk coerente com as perguntas; armazene metadados (t√≠tulo, se√ß√£o, fonte); fa√ßa avalia√ß√µes offline (precis√£o/recall, utilidade).</li>
      </ul>
      <pre><code># Exemplo conceitual (pseudoc√≥digo)
docs = split_into_chunks(load_corpus(), tokens=600)
vecs = embed(docs)
index = build_vector_index(vecs)

query = embed("Como normalizar tabelas?")
tops = index.search(query, k=5)
prompt = compose(system, user, context=tops)
answer = llm.generate(prompt)
      </code></pre>
    </section>

    <section class="section">
      <h2>pgvector no PostgreSQL</h2>
      <p>Armazene embeddings diretamente no PostgreSQL e fa√ßa busca por similaridade. Por qu√™: integra dados relacionais e sem√¢nticos no mesmo SGBD. Como: extens√£o <code>pgvector</code>.</p>
      <pre><code>-- Instalar extens√£o (exige permiss√µes de superuser)
CREATE EXTENSION IF NOT EXISTS vector;

-- Tabela com coluna vetorial (dimens√£o 1536 como exemplo)
CREATE TABLE doc_chunk (
  id BIGSERIAL PRIMARY KEY,
  content TEXT NOT NULL,
  embedding VECTOR(1536)
);

-- Inserir: gere o embedding na app e envie como array
INSERT INTO doc_chunk (content, embedding)
VALUES (
  'Normaliza√ß√£o at√© 3FN reduz redund√¢ncia',
  '[0.012, -0.034, ...]'::vector
);

-- Consulta por similaridade (menor dist√¢ncia = mais similar)
-- <->: dist√¢ncia euclidiana | <#>: inner product | <=>: cosine
WITH q AS (
  SELECT '[0.010, -0.020, ...]'::vector AS v
)
SELECT id, content
FROM doc_chunk, q
ORDER BY embedding <=> q.v
LIMIT 5;
      </code></pre>
      <div class="note">Boas pr√°ticas: mantenha metadados (fonte, se√ß√£o), crie √≠ndice <code>ivfflat</code> para escala e ajuste <code>lists</code>/<code>probes</code> conforme recall/lat√™ncia.</div>
    </section>
  </main>
  <footer>
    <p class="muted">FDB ‚Äî IA aplicada ‚Ä¢ Contato: <a href="mailto:inematds@gmail.com">inematds@gmail.com</a></p>
  </footer>
</body>
</html>
